{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of content:\n* Task contrains\n* Notes of the dataset\n* Data dictionary\n* Basic EDA\n* Simple models\n* Paper's CNN model re-typed and compared to the best simple model\n* Experiment to check the importance of window size\n* Some thoughts about the binary classification trade-offs\n* Paper's FastFourierTransformed data with FCN\n* Why did FFT - FCN perform so well, domain knowledge","metadata":{}},{"cell_type":"markdown","source":"## Task constrains  \n* Use only deeplearning   \n* Use tensorflow \n* Use domain knowledge\n* Don't make computationally more expensive models than the paper has","metadata":{}},{"cell_type":"markdown","source":"## Notes about the dataset\n* Balanced multiclass classification, later re-created as unbalanced binary classification  \n* Distribution shift (there is some difference between development and evaluation data)  \n* Large dataset, 26m datarows per class (or features per feature vectors)  \n* Structured data  \n* Offline learning (all data is available, no new data is expected for re-training)\n* Raw data and tidy data","metadata":{}},{"cell_type":"markdown","source":"## Data dictionary\n* V_in         : The input voltage to the motor controller V_in (in V)  \n* Measured_RPM   : The rotation speed of the motor (in RPM; computed from speed measurements using the DT9837)  \n* Vibration_1      : The signal from the first vibration sensor  \n* Vibration_2     : The signal from the second vibration sensor  \n* Vibration_3     : The signal from the third vibration sensor  \n\nCheck the paper https://arxiv.org/pdf/2005.12742.pdf for the details of the measurement setup","metadata":{}},{"cell_type":"markdown","source":"Note: don't run the whole notebook at once, it'll crash","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:06:38.758198Z","iopub.execute_input":"2022-04-10T18:06:38.7586Z","iopub.status.idle":"2022-04-10T18:06:38.796927Z","shell.execute_reply.started":"2022-04-10T18:06:38.758517Z","shell.execute_reply":"2022-04-10T18:06:38.796169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: you need 0E, 4E, 0D, 4D csv files loaded in to run this part","metadata":{}},{"cell_type":"code","source":"skip = 50000\n\ndata_0D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0D.csv', skiprows = skip, header=None)\n#data_1D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1D.csv', skiprows = skip, header=None)\n#data_2D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2D.csv', skiprows = skip, header=None)\n#data_3D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3D.csv', skiprows = skip, header=None)\ndata_4D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4D.csv', skiprows = skip, header=None)\n\ndata_0E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0E.csv', skiprows = skip, header=None)\n#data_1E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1E.csv', skiprows = skip, header=None)\n#data_2E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2E.csv', skiprows = skip, header=None)\n#data_3E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3E.csv', skiprows = skip, header=None)\ndata_4E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4E.csv', skiprows = skip, header=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:06:38.798618Z","iopub.execute_input":"2022-04-10T18:06:38.799127Z","iopub.status.idle":"2022-04-10T18:07:58.389825Z","shell.execute_reply.started":"2022-04-10T18:06:38.799092Z","shell.execute_reply":"2022-04-10T18:07:58.389047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's do some basic EDA (exploratory data analysis)","metadata":{}},{"cell_type":"markdown","source":"#### Start with shapes ","metadata":{}},{"cell_type":"code","source":"print(f\"Shape of train data '0D.csv' : {data_0D_train.shape}\")\nprint(f\"Shape of test  data '0E.csv' : {data_0E_test.shape}\\n\")\n\nprint(f\"\\nShape of train data '4D.csv' : {data_4D_train.shape}\")\nprint(f\"Shape of test  data '4E.csv' : {data_4E_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:58.391523Z","iopub.execute_input":"2022-04-10T18:07:58.391825Z","iopub.status.idle":"2022-04-10T18:07:58.39802Z","shell.execute_reply.started":"2022-04-10T18:07:58.391771Z","shell.execute_reply":"2022-04-10T18:07:58.397284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check missing values","metadata":{}},{"cell_type":"code","source":"data_0D_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:58.399422Z","iopub.execute_input":"2022-04-10T18:07:58.399923Z","iopub.status.idle":"2022-04-10T18:07:58.700211Z","shell.execute_reply.started":"2022-04-10T18:07:58.399887Z","shell.execute_reply":"2022-04-10T18:07:58.699399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0E_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:58.702589Z","iopub.execute_input":"2022-04-10T18:07:58.702849Z","iopub.status.idle":"2022-04-10T18:07:58.783127Z","shell.execute_reply.started":"2022-04-10T18:07:58.702814Z","shell.execute_reply":"2022-04-10T18:07:58.782306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_4D_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:58.784648Z","iopub.execute_input":"2022-04-10T18:07:58.784917Z","iopub.status.idle":"2022-04-10T18:07:59.071756Z","shell.execute_reply.started":"2022-04-10T18:07:58.784882Z","shell.execute_reply":"2022-04-10T18:07:59.070762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_4E_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:59.073349Z","iopub.execute_input":"2022-04-10T18:07:59.073609Z","iopub.status.idle":"2022-04-10T18:07:59.156608Z","shell.execute_reply.started":"2022-04-10T18:07:59.073572Z","shell.execute_reply":"2022-04-10T18:07:59.155801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values, excellent","metadata":{}},{"cell_type":"markdown","source":"### Some univariate and bivariate analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:59.1581Z","iopub.execute_input":"2022-04-10T18:07:59.158367Z","iopub.status.idle":"2022-04-10T18:07:59.162399Z","shell.execute_reply.started":"2022-04-10T18:07:59.158329Z","shell.execute_reply":"2022-04-10T18:07:59.161558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start with the first subset pair 0D, 0E, these are the balanced samples","metadata":{}},{"cell_type":"code","source":"# sample a smaller dataset for faster checks, 20% should be enough\ndata_0D_train_sampled = data_0D_train[::5]\ndata_0D_train_sampled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:59.163522Z","iopub.execute_input":"2022-04-10T18:07:59.164007Z","iopub.status.idle":"2022-04-10T18:07:59.174647Z","shell.execute_reply.started":"2022-04-10T18:07:59.163967Z","shell.execute_reply":"2022-04-10T18:07:59.1739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0E_test_sampled = data_0E_test[::5]\ndata_0E_test_sampled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:59.175985Z","iopub.execute_input":"2022-04-10T18:07:59.176236Z","iopub.status.idle":"2022-04-10T18:07:59.183532Z","shell.execute_reply.started":"2022-04-10T18:07:59.176202Z","shell.execute_reply":"2022-04-10T18:07:59.182691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check the distribution","metadata":{}},{"cell_type":"code","source":"column_0 = data_0D_train_sampled.iloc[:, 0]\nplt.plot(column_0)\nplt.title(\"input voltage per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"V input\");\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:07:59.187823Z","iopub.execute_input":"2022-04-10T18:07:59.188168Z","iopub.status.idle":"2022-04-10T18:08:00.412539Z","shell.execute_reply.started":"2022-04-10T18:07:59.188136Z","shell.execute_reply":"2022-04-10T18:08:00.410859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_0 = data_0D_train_sampled.iloc[:, 1]\nplt.plot(column_0);\n\nplt.title(\"rpm voltage per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"rpm\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:00.416135Z","iopub.execute_input":"2022-04-10T18:08:00.416729Z","iopub.status.idle":"2022-04-10T18:08:01.456563Z","shell.execute_reply.started":"2022-04-10T18:08:00.416689Z","shell.execute_reply":"2022-04-10T18:08:01.455885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_1 = data_0D_train_sampled.iloc[:,2]\nplt.plot(column_1);\nplt.title(\"vibration sensor 1 measurements per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"vibration sensor 1 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:01.457902Z","iopub.execute_input":"2022-04-10T18:08:01.45833Z","iopub.status.idle":"2022-04-10T18:08:02.582777Z","shell.execute_reply.started":"2022-04-10T18:08:01.45829Z","shell.execute_reply":"2022-04-10T18:08:02.582068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_2 = data_0D_train_sampled.iloc[:, 3]\nplt.plot(column_2);\nplt.title(\"vibration sensor 2 measurements per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"vibration sensor 2 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:02.584178Z","iopub.execute_input":"2022-04-10T18:08:02.584647Z","iopub.status.idle":"2022-04-10T18:08:03.696357Z","shell.execute_reply.started":"2022-04-10T18:08:02.584609Z","shell.execute_reply":"2022-04-10T18:08:03.695666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_3 = data_0D_train_sampled.iloc[:, 4]\nplt.plot(column_3);\n\nplt.title(\"vibration sensor 3 measurements per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"vibration sensor 3 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:03.697633Z","iopub.execute_input":"2022-04-10T18:08:03.698032Z","iopub.status.idle":"2022-04-10T18:08:04.787366Z","shell.execute_reply.started":"2022-04-10T18:08:03.697993Z","shell.execute_reply":"2022-04-10T18:08:04.786672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some outliners and deeplearning models are sensitive to those.","metadata":{}},{"cell_type":"markdown","source":"#### Check the correlation matrix on the training data","metadata":{}},{"cell_type":"code","source":"# sometimes it throws error\nplt.matshow(data_0D_train_sampled.corr())\nplt.title(\"Correlation matrix\")\nplt.xlabel(\"V in, rpm, vibration sensor 1, 2, 3 measurements\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:04.788676Z","iopub.execute_input":"2022-04-10T18:08:04.789107Z","iopub.status.idle":"2022-04-10T18:08:05.590376Z","shell.execute_reply.started":"2022-04-10T18:08:04.789067Z","shell.execute_reply":"2022-04-10T18:08:05.589723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High correlation between row 0 and 1 (rpm and Vin according to the data dictionary)\nNothing suprising here, as:\n\nVoltage on the coils (armature) * machine constant = RPM\n\nStill, don't forget that the Vin in the dataset is the motor controller's input, not the coltage on the coils.\n\nThere is a slight correlation between the 2 and 4 (1st and 3rd vibration sensor). If you check the measurement setup it's quite interesing. I assumed correlation between sensor 2 and 3. It turns out I was wrong.","metadata":{}},{"cell_type":"markdown","source":"#### Let's do the same with the test dataset","metadata":{}},{"cell_type":"code","source":"column_0 = data_0E_test_sampled.iloc[:, 0]\nplt.plot(column_0);\n\nplt.title(\"input voltage per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"V input\");\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:05.591691Z","iopub.execute_input":"2022-04-10T18:08:05.592185Z","iopub.status.idle":"2022-04-10T18:08:05.99285Z","shell.execute_reply.started":"2022-04-10T18:08:05.592139Z","shell.execute_reply":"2022-04-10T18:08:05.992099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_1 = data_0E_test_sampled.iloc[:,1]\nplt.plot(column_1)\n\nplt.title(\"rpm voltage per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"rpm\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:05.994156Z","iopub.execute_input":"2022-04-10T18:08:05.994581Z","iopub.status.idle":"2022-04-10T18:08:06.378739Z","shell.execute_reply.started":"2022-04-10T18:08:05.994541Z","shell.execute_reply":"2022-04-10T18:08:06.378021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_2 = data_0E_test_sampled.iloc[:,2]\nplt.plot(column_2)\n\nplt.title(\"vibration sensor 1 measurements per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"vibration sensor 1 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:06.379992Z","iopub.execute_input":"2022-04-10T18:08:06.380405Z","iopub.status.idle":"2022-04-10T18:08:06.900857Z","shell.execute_reply.started":"2022-04-10T18:08:06.380357Z","shell.execute_reply":"2022-04-10T18:08:06.900192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_3 = data_0E_test_sampled.iloc[:,3]\nplt.plot(column_3)\n\nplt.title(\"vibration sensor 2 measurements per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"vibration sensor 2 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:06.902173Z","iopub.execute_input":"2022-04-10T18:08:06.902626Z","iopub.status.idle":"2022-04-10T18:08:07.388922Z","shell.execute_reply.started":"2022-04-10T18:08:06.902584Z","shell.execute_reply":"2022-04-10T18:08:07.388262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_4 = data_0E_test_sampled.iloc[:,4]\nplt.plot(column_4)\n\nplt.title(\"vibration sensor 3 measurements per sample\")\nplt.xlabel(\"sample\")\nplt.ylabel(\"vibration sensor 3 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:07.390166Z","iopub.execute_input":"2022-04-10T18:08:07.39059Z","iopub.status.idle":"2022-04-10T18:08:07.871454Z","shell.execute_reply.started":"2022-04-10T18:08:07.390551Z","shell.execute_reply":"2022-04-10T18:08:07.870774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation matrix on test data","metadata":{}},{"cell_type":"code","source":"plt.matshow(data_0D_train_sampled.corr())\n\nplt.title(\"Correlation matrix\")\nplt.xlabel(\"V in, rpm, vibration sensor 1, 2, 3 measurements\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:07.872796Z","iopub.execute_input":"2022-04-10T18:08:07.873233Z","iopub.status.idle":"2022-04-10T18:08:08.66105Z","shell.execute_reply.started":"2022-04-10T18:08:07.873191Z","shell.execute_reply":"2022-04-10T18:08:08.660118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I should go through the 4D, 4E subset pair, let's just check the rpm feature and the correlation matrix","metadata":{}},{"cell_type":"code","source":"data_4D_train_sampled = data_4D_train[::5]\n\ndata_4E_test_sampled = data_4E_test[::5]\ndata_4D_train_sampled.shape, data_4E_test_sampled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:08.665439Z","iopub.execute_input":"2022-04-10T18:08:08.665725Z","iopub.status.idle":"2022-04-10T18:08:08.675204Z","shell.execute_reply.started":"2022-04-10T18:08:08.665687Z","shell.execute_reply":"2022-04-10T18:08:08.673574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_0 = data_4E_test_sampled.iloc[:, 1]\nplt.plot(column_0);","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:08.676421Z","iopub.execute_input":"2022-04-10T18:08:08.676726Z","iopub.status.idle":"2022-04-10T18:08:09.0808Z","shell.execute_reply.started":"2022-04-10T18:08:08.676687Z","shell.execute_reply":"2022-04-10T18:08:09.080101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_0 = data_4D_train_sampled.iloc[:, 1]\nplt.plot(column_0);","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:09.081961Z","iopub.execute_input":"2022-04-10T18:08:09.083431Z","iopub.status.idle":"2022-04-10T18:08:10.038766Z","shell.execute_reply.started":"2022-04-10T18:08:09.083386Z","shell.execute_reply":"2022-04-10T18:08:10.038104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More outliners here","metadata":{}},{"cell_type":"markdown","source":"At this point I read the paper's relevant part to check their figures. I didn't miss much, although downsampling the data for the figures was probably a mistake. I could have missed the outliners.","metadata":{}},{"cell_type":"markdown","source":"## Data preparation for the simple models","metadata":{}},{"cell_type":"markdown","source":"#### Down-sampling the data and merging of the subsets","metadata":{}},{"cell_type":"code","source":"# let's start with 0.1% of the data for fast early experiments\ndata_0D_train_sampled = data_0D_train[::1000].copy()\ndata_0E_test_sampled = data_0E_test[::1000].copy()\ndata_4D_train_sampled = data_4D_train[::1000].copy()\ndata_4E_test_sampled = data_4E_test[::1000].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.039933Z","iopub.execute_input":"2022-04-10T18:08:10.040429Z","iopub.status.idle":"2022-04-10T18:08:10.050798Z","shell.execute_reply.started":"2022-04-10T18:08:10.04039Z","shell.execute_reply":"2022-04-10T18:08:10.050097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0D_train_sampled.shape, data_0E_test_sampled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.051852Z","iopub.execute_input":"2022-04-10T18:08:10.052221Z","iopub.status.idle":"2022-04-10T18:08:10.058803Z","shell.execute_reply.started":"2022-04-10T18:08:10.052183Z","shell.execute_reply":"2022-04-10T18:08:10.057963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_4D_train_sampled.shape, data_4E_test_sampled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.060316Z","iopub.execute_input":"2022-04-10T18:08:10.060723Z","iopub.status.idle":"2022-04-10T18:08:10.069701Z","shell.execute_reply.started":"2022-04-10T18:08:10.060685Z","shell.execute_reply":"2022-04-10T18:08:10.069027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0D_train_sampled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.076Z","iopub.execute_input":"2022-04-10T18:08:10.076184Z","iopub.status.idle":"2022-04-10T18:08:10.083811Z","shell.execute_reply.started":"2022-04-10T18:08:10.076161Z","shell.execute_reply":"2022-04-10T18:08:10.08235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Supervised learning\nLet's turn the dataset into a binary classification dataset\nfirst add target values, 0 for the balanced and 1 unbalanced labels","metadata":{}},{"cell_type":"code","source":"# Add labels for the binary classification (supervised learning) \n\ndata_0D_train_sampled['Target']= 0.0\ndata_0E_test_sampled['Target']= 0.0     \ndata_4D_train_sampled['Target']= 1.0       \ndata_4E_test_sampled['Target']= 1.0","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.085544Z","iopub.execute_input":"2022-04-10T18:08:10.086263Z","iopub.status.idle":"2022-04-10T18:08:10.100121Z","shell.execute_reply.started":"2022-04-10T18:08:10.086176Z","shell.execute_reply":"2022-04-10T18:08:10.099267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results\ndata_0D_train_sampled.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.101016Z","iopub.execute_input":"2022-04-10T18:08:10.101777Z","iopub.status.idle":"2022-04-10T18:08:10.120305Z","shell.execute_reply.started":"2022-04-10T18:08:10.101735Z","shell.execute_reply":"2022-04-10T18:08:10.119668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_4D_train_sampled.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.121453Z","iopub.execute_input":"2022-04-10T18:08:10.121758Z","iopub.status.idle":"2022-04-10T18:08:10.135327Z","shell.execute_reply.started":"2022-04-10T18:08:10.121723Z","shell.execute_reply":"2022-04-10T18:08:10.134448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ignore index because it's the same for both datasets\nbinary_training_ds = pd.concat([data_0D_train_sampled, data_4D_train_sampled], names=None, ignore_index=True, sort=False)\nbinary_training_ds.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.136972Z","iopub.execute_input":"2022-04-10T18:08:10.13748Z","iopub.status.idle":"2022-04-10T18:08:10.148286Z","shell.execute_reply.started":"2022-04-10T18:08:10.137444Z","shell.execute_reply":"2022-04-10T18:08:10.147396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the subsets\n# Ignore index because it's the same for both datasets\nbinary_test_ds = pd.concat([data_0E_test_sampled, data_4E_test_sampled], names=None, ignore_index=True, sort=False)\nbinary_test_ds.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.149675Z","iopub.execute_input":"2022-04-10T18:08:10.149921Z","iopub.status.idle":"2022-04-10T18:08:10.157817Z","shell.execute_reply.started":"2022-04-10T18:08:10.149889Z","shell.execute_reply":"2022-04-10T18:08:10.157099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle the training data\nnp.random.seed(42)\nbinary_training_ds = binary_training_ds.sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.15932Z","iopub.execute_input":"2022-04-10T18:08:10.159871Z","iopub.status.idle":"2022-04-10T18:08:10.169785Z","shell.execute_reply.started":"2022-04-10T18:08:10.159832Z","shell.execute_reply":"2022-04-10T18:08:10.169042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scale the features, encode the labels","metadata":{}},{"cell_type":"code","source":"# create target dataset for training data\nbinary_training_labels = binary_training_ds[\"Target\"]\nbinary_training_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.17119Z","iopub.execute_input":"2022-04-10T18:08:10.171716Z","iopub.status.idle":"2022-04-10T18:08:10.18043Z","shell.execute_reply.started":"2022-04-10T18:08:10.171682Z","shell.execute_reply":"2022-04-10T18:08:10.179672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create target dataset for test data\nbinary_test_labels = binary_test_ds[\"Target\"]\nbinary_test_labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.181386Z","iopub.execute_input":"2022-04-10T18:08:10.183054Z","iopub.status.idle":"2022-04-10T18:08:10.191717Z","shell.execute_reply.started":"2022-04-10T18:08:10.183004Z","shell.execute_reply":"2022-04-10T18:08:10.190807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create features dataset for training data\nbinary_training_features = binary_training_ds.drop([\"Target\"], axis=1)\nbinary_training_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.193126Z","iopub.execute_input":"2022-04-10T18:08:10.193624Z","iopub.status.idle":"2022-04-10T18:08:10.207195Z","shell.execute_reply.started":"2022-04-10T18:08:10.193589Z","shell.execute_reply":"2022-04-10T18:08:10.206395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create features dataset for test data\nbinary_test_features = binary_test_ds.drop([\"Target\"], axis=1)\nbinary_test_features.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.208477Z","iopub.execute_input":"2022-04-10T18:08:10.20884Z","iopub.status.idle":"2022-04-10T18:08:10.22164Z","shell.execute_reply.started":"2022-04-10T18:08:10.208804Z","shell.execute_reply":"2022-04-10T18:08:10.220886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\n# Normalize the data\n\n# Create column transformer (this will help us normalize/preprocess our data)\nct = make_column_transformer(\n    (MinMaxScaler(), [0, 1, 2, 3, 4])) # get all values between 0 and 1\n\n# Fit column transformer on the training data only (doing so on test data would result in data leakage)\nct.fit(binary_training_features)\n    \n# Transform training and test data with normalization (MinMaxScalar) \nbinary_training_features_normal = ct.transform(binary_training_features)\nbinary_test_features_normal = ct.transform(binary_test_features)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:10.22304Z","iopub.execute_input":"2022-04-10T18:08:10.224055Z","iopub.status.idle":"2022-04-10T18:08:11.204977Z","shell.execute_reply.started":"2022-04-10T18:08:10.224014Z","shell.execute_reply":"2022-04-10T18:08:11.204109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the results\npd.DataFrame(binary_training_features_normal).head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.206217Z","iopub.execute_input":"2022-04-10T18:08:11.206756Z","iopub.status.idle":"2022-04-10T18:08:11.219561Z","shell.execute_reply.started":"2022-04-10T18:08:11.20672Z","shell.execute_reply":"2022-04-10T18:08:11.21879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(binary_training_features_normal).tail()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.220761Z","iopub.execute_input":"2022-04-10T18:08:11.221421Z","iopub.status.idle":"2022-04-10T18:08:11.247862Z","shell.execute_reply.started":"2022-04-10T18:08:11.221387Z","shell.execute_reply":"2022-04-10T18:08:11.245775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(binary_test_features_normal).head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.249229Z","iopub.execute_input":"2022-04-10T18:08:11.249519Z","iopub.status.idle":"2022-04-10T18:08:11.269292Z","shell.execute_reply.started":"2022-04-10T18:08:11.249486Z","shell.execute_reply":"2022-04-10T18:08:11.26834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's use shorter, standard variable names...\nX_train = pd.DataFrame(binary_training_features_normal)\nX_test = pd.DataFrame(binary_test_features_normal)\n\ny_train = pd.DataFrame(binary_training_labels)\ny_test = pd.DataFrame(binary_test_labels)\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.273864Z","iopub.execute_input":"2022-04-10T18:08:11.276477Z","iopub.status.idle":"2022-04-10T18:08:11.299938Z","shell.execute_reply.started":"2022-04-10T18:08:11.276408Z","shell.execute_reply":"2022-04-10T18:08:11.299216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.304205Z","iopub.execute_input":"2022-04-10T18:08:11.306488Z","iopub.status.idle":"2022-04-10T18:08:11.316009Z","shell.execute_reply.started":"2022-04-10T18:08:11.306451Z","shell.execute_reply":"2022-04-10T18:08:11.315166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.320813Z","iopub.execute_input":"2022-04-10T18:08:11.32217Z","iopub.status.idle":"2022-04-10T18:08:11.338017Z","shell.execute_reply.started":"2022-04-10T18:08:11.322105Z","shell.execute_reply":"2022-04-10T18:08:11.337294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple models 0.1% data, all features","metadata":{}},{"cell_type":"raw","source":"DNN, it's structured data\nCNN, it's audio like data\nLSTM it's a sequenced data, \n\n0.1% data, binary classification, all features\n1% data, binary classification, all features\n\nbaseline result is about 80% acc\n\nNote: LSTM model would require a different data preparation, without shuffling, something I won't do for the simple models. I give it a try anyway.","metadata":{}},{"cell_type":"code","source":"# Trouble shooting\nX_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.342108Z","iopub.execute_input":"2022-04-10T18:08:11.344448Z","iopub.status.idle":"2022-04-10T18:08:11.363753Z","shell.execute_reply.started":"2022-04-10T18:08:11.344412Z","shell.execute_reply":"2022-04-10T18:08:11.363058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.367295Z","iopub.execute_input":"2022-04-10T18:08:11.369177Z","iopub.status.idle":"2022-04-10T18:08:11.384032Z","shell.execute_reply.started":"2022-04-10T18:08:11.369142Z","shell.execute_reply":"2022-04-10T18:08:11.383414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import models, layers\nimport time # to measure how long training takes","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:11.387564Z","iopub.execute_input":"2022-04-10T18:08:11.389424Z","iopub.status.idle":"2022-04-10T18:08:16.431332Z","shell.execute_reply.started":"2022-04-10T18:08:11.389388Z","shell.execute_reply":"2022-04-10T18:08:16.430607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42) # for reproducibility\nstart = time.time()\n\nmodel_0_DNN = models.Sequential([\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel_0_DNN.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                   metrics=\"accuracy\")\n\nhistory_0 = model_0_DNN.fit(X_train, \n                            y_train, \n                            epochs = 15, \n                            validation_data=(X_test, y_test))\n\nend = time.time()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:08:16.432795Z","iopub.execute_input":"2022-04-10T18:08:16.433095Z","iopub.status.idle":"2022-04-10T18:09:29.080068Z","shell.execute_reply.started":"2022-04-10T18:08:16.433056Z","shell.execute_reply":"2022-04-10T18:09:29.079293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = end - start\nprint(f\"the training took {temp} seconds\")\n\n# Plot history (also known as a loss curve)\npd.DataFrame(history_0.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:09:29.081345Z","iopub.execute_input":"2022-04-10T18:09:29.082079Z","iopub.status.idle":"2022-04-10T18:09:29.330898Z","shell.execute_reply.started":"2022-04-10T18:09:29.082038Z","shell.execute_reply":"2022-04-10T18:09:29.330178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The simple DNN model seems to overfit at around 10 epochs, learning rate is too high","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nstart = time.time()\n\nmodel_1_CNN = models.Sequential([\n    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements\n    layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n    layers.Flatten(),\n    layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel_1_CNN.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                    metrics=\"accuracy\")\n\nhistory_1 = model_1_CNN.fit(X_train, \n                            y_train, \n                            epochs = 15, \n                            validation_data=(X_test, y_test))\n\nend = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:09:29.332328Z","iopub.execute_input":"2022-04-10T18:09:29.332792Z","iopub.status.idle":"2022-04-10T18:10:48.229306Z","shell.execute_reply.started":"2022-04-10T18:09:29.332756Z","shell.execute_reply":"2022-04-10T18:10:48.228603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = end - start\nprint(f\"the training took {temp} seconds\")\n\n# Plot history (also known as a loss curve)\npd.DataFrame(history_1.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:10:48.230582Z","iopub.execute_input":"2022-04-10T18:10:48.230827Z","iopub.status.idle":"2022-04-10T18:10:48.478076Z","shell.execute_reply.started":"2022-04-10T18:10:48.230793Z","shell.execute_reply":"2022-04-10T18:10:48.47654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the CNN models could be trained for a few extra epochs, learning rate is too high","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nstart = time.time()\n\nmodel_2_LSTM = models.Sequential([\n    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # expand input dimension to be compatible with LSTM\n    layers.LSTM(128, activation=\"relu\"),\n    layers.Dense(1, activation = \"sigmoid\"),\n])\n\nmodel_2_LSTM.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                    metrics=\"accuracy\")\n\nhistory_2 = model_2_LSTM.fit(X_train, \n                            y_train, \n                            epochs = 15, \n                            validation_data=(X_test, y_test))\n\nend = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:10:48.479452Z","iopub.execute_input":"2022-04-10T18:10:48.479683Z","iopub.status.idle":"2022-04-10T18:11:45.712916Z","shell.execute_reply.started":"2022-04-10T18:10:48.479649Z","shell.execute_reply":"2022-04-10T18:11:45.709474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = end - start\nprint(f\"the training took {temp} seconds\")\n\n# Plot history (also known as a loss curve)\npd.DataFrame(history_2.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.714659Z","iopub.status.idle":"2022-04-10T18:11:45.715047Z","shell.execute_reply.started":"2022-04-10T18:11:45.714844Z","shell.execute_reply":"2022-04-10T18:11:45.714868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSTM is same as CNN, some additional epoch could help, learning rate seems to be too high","metadata":{}},{"cell_type":"markdown","source":"#### summary of simple models 0.1% data, all features","metadata":{}},{"cell_type":"markdown","source":"All three has similar results, either the downsampled dataset is too small, the models are too simple, maybe the data isn't information rich enough.  \n\nOn the other hand the DNN model was trained in less time, thus the cheapest option.\n\nThe fact that all three models have very similar results leads to too small dataset.\nIt's an easy fix.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Simple models 1% data, all features","metadata":{}},{"cell_type":"markdown","source":"### Data preparation, now in one block","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\n# let's start with 1% of the data for fast early experiments\ndata_0D_train_sampled = data_0D_train[::100].copy()\ndata_0E_test_sampled = data_0E_test[::100].copy()\ndata_4D_train_sampled = data_4D_train[::100].copy()\ndata_4E_test_sampled = data_4E_test[::100].copy()\n\n# add labels for the binary classification (supervised learning) \n\ndata_0D_train_sampled['Target']= 0.0\ndata_0E_test_sampled['Target']= 0.0     \ndata_4D_train_sampled['Target']= 1.0       \ndata_4E_test_sampled['Target']= 1.0\n\n# ignore index because it's the same for both datasets\nbinary_training_ds = pd.concat([data_0D_train_sampled, data_4D_train_sampled], names=None, ignore_index=True, sort=False)\n\n# merge the subsets\n\n# ignore index because it's the same for both datasets\nbinary_test_ds = pd.concat([data_0E_test_sampled, data_4E_test_sampled], names=None, ignore_index=True, sort=False)\n\n# shuffle the training data\nbinary_training_ds = binary_training_ds.sample(frac=1)\n\n# create target dataset for training data\nbinary_training_labels = binary_training_ds[\"Target\"]\n\n# create target dataset for test data\nbinary_test_labels = binary_test_ds[\"Target\"]\n\n# create features dataset for training data\nbinary_training_features = binary_training_ds.drop([\"Target\"], axis=1)\n\n# create features dataset for test data\nbinary_test_features = binary_test_ds.drop([\"Target\"], axis=1)\n\n# normalize the data\n\n# Create column transformer (this will help us normalize/preprocess our data)\nct = make_column_transformer(\n    (MinMaxScaler(), [0, 1, 2, 3, 4])) # get all values between 0 and 1\n\n# Fit column transformer on the training data only (doing so on test data would result in data leakage)\nct.fit(binary_training_features)\n    \n# Transform training and test data with normalization (MinMaxScalar) \nbinary_training_features_normal = ct.transform(binary_training_features)\nbinary_test_features_normal = ct.transform(binary_test_features)\n\n# let's use shorter, standard variable names...\nX_train = pd.DataFrame(binary_training_features_normal)\nX_test = pd.DataFrame(binary_test_features_normal)\n\ny_train = pd.DataFrame(binary_training_labels)\ny_test = pd.DataFrame(binary_test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.716358Z","iopub.status.idle":"2022-04-10T18:11:45.716915Z","shell.execute_reply.started":"2022-04-10T18:11:45.716672Z","shell.execute_reply":"2022-04-10T18:11:45.716697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same DNN, CNN, LSTM model as before with early stopping callback, it takes longer to train and I don't want overfitting  \n\nAnother change is the batch_size. In the dataset the motor revolves at least 600 time a minute (rpm), that's 10 revolution per second (or per 4096 sample). In 1024 sample there should be at least 2-3 revolution, probably more than enough.","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42) # for reproducibility\nstart = time.time()\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel_3_DNN = models.Sequential([\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel_3_DNN.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                   metrics=\"accuracy\")\n\nhistory_3 = model_3_DNN.fit(X_train, \n                            y_train, \n                            epochs = 100, \n                            batch_size = 1024,\n                            validation_data=(X_test, y_test),\n                            callbacks=[callback])\n\nend = time.time()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.717972Z","iopub.status.idle":"2022-04-10T18:11:45.718863Z","shell.execute_reply.started":"2022-04-10T18:11:45.718623Z","shell.execute_reply":"2022-04-10T18:11:45.718648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = end - start\nprint(f\"the training took {temp} seconds\")\n\n# Plot history (also known as a loss curve)\npd.DataFrame(history_3.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.720139Z","iopub.status.idle":"2022-04-10T18:11:45.720556Z","shell.execute_reply.started":"2022-04-10T18:11:45.720336Z","shell.execute_reply":"2022-04-10T18:11:45.720359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nstart = time.time()\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel_4_CNN = models.Sequential([\n    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements\n    layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n    layers.Flatten(),\n    layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel_4_CNN.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                    metrics=\"accuracy\")\n\nhistory_4 = model_4_CNN.fit(X_train, \n                            y_train, \n                            epochs = 100, \n                            batch_size = 1024,\n                            validation_data=(X_test, y_test),\n                            callbacks=[callback])\n\nend = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.7216Z","iopub.status.idle":"2022-04-10T18:11:45.722422Z","shell.execute_reply.started":"2022-04-10T18:11:45.722173Z","shell.execute_reply":"2022-04-10T18:11:45.7222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = end - start\nprint(f\"the training took {temp} seconds\")\n\n# Plot history (also known as a loss curve)\npd.DataFrame(history_4.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.723665Z","iopub.status.idle":"2022-04-10T18:11:45.724092Z","shell.execute_reply.started":"2022-04-10T18:11:45.723854Z","shell.execute_reply":"2022-04-10T18:11:45.723878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nstart = time.time()\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel_5_LSTM = models.Sequential([\n    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # expand input dimension to be compatible with LSTM\n    layers.LSTM(128, activation=\"relu\"),\n    layers.Dense(1, activation = \"sigmoid\"),\n])\n\nmodel_5_LSTM.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                    metrics=\"accuracy\")\n\nhistory_5 = model_5_LSTM.fit(X_train, \n                            y_train, \n                            epochs = 100, \n                            batch_size = 1024,\n                            validation_data=(X_test, y_test),\n                            callbacks=[callback])\n\nend = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.725289Z","iopub.status.idle":"2022-04-10T18:11:45.725911Z","shell.execute_reply.started":"2022-04-10T18:11:45.725661Z","shell.execute_reply":"2022-04-10T18:11:45.725685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = end - start\nprint(f\"the training took {temp} seconds\")\n\n# Plot history (also known as a loss curve)\npd.DataFrame(history_5.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:11:45.727204Z","iopub.status.idle":"2022-04-10T18:11:45.727622Z","shell.execute_reply.started":"2022-04-10T18:11:45.727404Z","shell.execute_reply":"2022-04-10T18:11:45.727428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary simple models, 1% data, all features\nThe larger batch size (1024 instead of the default 32) improved the speed of the training, it took about the same time to train with 10x data.  \n\nThe DNN scaled the best, the CNN and the LSTM models didn't improve.   \n\nNormally I'd try more complex models here, but the goal was just to try out some simple models, so I turn my attention to the paper.","metadata":{}},{"cell_type":"markdown","source":"Note: using the same variables at the data preparation part is probably a bad practice.","metadata":{}},{"cell_type":"markdown","source":"finished here","metadata":{}},{"cell_type":"markdown","source":"### Re-creation of the CNN architecture from the paper ","metadata":{}},{"cell_type":"markdown","source":"Note: restart the session to avoid crash at this point.  \nUse run/stop session and run after on this block.","metadata":{}},{"cell_type":"markdown","source":"The windowing part is from the paper's github notebook. I haven't checked the rest of the notebook at this point.   \n\nI'll guess the hyperparameters from a different time forecasting notebook (https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/10_time_series_forecasting_in_tensorflow.ipynb),and later repeat the experiment with the hyperparameters from the paper's github notebook.\n\nI only use the D0-D4, E0-E4, data subset pair here","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.utils import plot_model\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:17:39.791765Z","iopub.execute_input":"2022-04-10T18:17:39.792034Z","iopub.status.idle":"2022-04-10T18:17:39.796864Z","shell.execute_reply.started":"2022-04-10T18:17:39.792003Z","shell.execute_reply":"2022-04-10T18:17:39.796115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skip = 50000\n\ndata_0D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0D.csv', skiprows = skip, header=None)\n#data_1D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1D.csv', skiprows = skip, header=None)\n#data_2D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2D.csv', skiprows = skip, header=None)\n#data_3D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3D.csv', skiprows = skip, header=None)\ndata_4D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4D.csv', skiprows = skip, header=None)\n\ndata_0E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0E.csv', skiprows = skip, header=None)\n#data_1E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1E.csv', skiprows = skip, header=None)\n#data_2E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2E.csv', skiprows = skip, header=None)\n#data_3E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3E.csv', skiprows = skip, header=None)\ndata_4E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4E.csv', skiprows = skip, header=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:17:41.337765Z","iopub.execute_input":"2022-04-10T18:17:41.338022Z","iopub.status.idle":"2022-04-10T18:18:32.260144Z","shell.execute_reply.started":"2022-04-10T18:17:41.337994Z","shell.execute_reply":"2022-04-10T18:18:32.259272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# due to the windowing I can use the whole dataset\ndata_0D_train_sampled = data_0D_train\ndata_0E_test_sampled = data_0E_test\ndata_4D_train_sampled = data_4D_train\ndata_4E_test_sampled = data_4E_test\n\n# let's use only vibration sensor 1\ndata_0D_train_sampled = data_0D_train_sampled[2] # 3rd column is the vibration 1 sensor data-feed\ndata_0E_test_sampled = data_0E_test_sampled[2]\ndata_4D_train_sampled = data_4D_train_sampled[2]\ndata_4E_test_sampled = data_4E_test_sampled[2]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.262053Z","iopub.execute_input":"2022-04-10T18:18:32.262325Z","iopub.status.idle":"2022-04-10T18:18:32.275575Z","shell.execute_reply.started":"2022-04-10T18:18:32.262288Z","shell.execute_reply":"2022-04-10T18:18:32.274805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0D_train_sampled.max()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.277158Z","iopub.execute_input":"2022-04-10T18:18:32.277574Z","iopub.status.idle":"2022-04-10T18:18:32.32049Z","shell.execute_reply.started":"2022-04-10T18:18:32.277537Z","shell.execute_reply":"2022-04-10T18:18:32.319712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0D_train_sampled.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.322563Z","iopub.execute_input":"2022-04-10T18:18:32.322809Z","iopub.status.idle":"2022-04-10T18:18:32.328727Z","shell.execute_reply.started":"2022-04-10T18:18:32.322776Z","shell.execute_reply":"2022-04-10T18:18:32.327881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for windowing the data\nwindow = 4096\ndef windowing(data, window):\n    n = int(np.floor(data.shape[0] / window)) # number of batches\n    data = data[:n * window]\n    data_reshaped = data.values.reshape(n, window)\n    return pd.DataFrame(data_reshaped)\nwindowing(data_0D_train_sampled, window)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.330291Z","iopub.execute_input":"2022-04-10T18:18:32.330691Z","iopub.status.idle":"2022-04-10T18:18:32.369419Z","shell.execute_reply.started":"2022-04-10T18:18:32.330656Z","shell.execute_reply":"2022-04-10T18:18:32.368799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# window the data\nwindow = 4096\ndata_0D_train_sampled = windowing(data_0D_train_sampled, window)\ndata_0E_test_sampled = windowing(data_0E_test_sampled, window)\ndata_4D_train_sampled = windowing(data_4D_train_sampled, window)\ndata_4E_test_sampled = windowing(data_4E_test_sampled, window)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.370485Z","iopub.execute_input":"2022-04-10T18:18:32.371031Z","iopub.status.idle":"2022-04-10T18:18:32.377694Z","shell.execute_reply.started":"2022-04-10T18:18:32.370997Z","shell.execute_reply":"2022-04-10T18:18:32.376893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for the binary classification\n\ndata_0D_train_sampled['Target']= 0.0\ndata_0E_test_sampled['Target']= 0.0     \ndata_4D_train_sampled['Target']= 1.0       \ndata_4E_test_sampled['Target']= 1.0","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.379154Z","iopub.execute_input":"2022-04-10T18:18:32.379424Z","iopub.status.idle":"2022-04-10T18:18:32.398569Z","shell.execute_reply.started":"2022-04-10T18:18:32.379391Z","shell.execute_reply":"2022-04-10T18:18:32.397929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ignore index because it's the same for both datasets\nbinary_training_ds = pd.concat([data_0D_train_sampled, data_4D_train_sampled], names=None, ignore_index=True, sort=False)\nbinary_test_ds = pd.concat([data_0E_test_sampled, data_4E_test_sampled], names=None, ignore_index=True, sort=False)\n\n# get labels\nbinary_training_labels = binary_training_ds[\"Target\"]\nbinary_test_labels = binary_test_ds[\"Target\"]\n# get features\nbinary_training_features = binary_training_ds.drop([\"Target\"], axis=1)\nbinary_test_features = binary_test_ds.drop([\"Target\"], axis=1)\n\n\n# let's use shorter variable names...\nX_train = pd.DataFrame(binary_training_features)\nX_test = pd.DataFrame(binary_test_features)\n\ny_train = pd.DataFrame(binary_training_labels)\ny_test = pd.DataFrame(binary_test_labels)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:32.399783Z","iopub.execute_input":"2022-04-10T18:18:32.400019Z","iopub.status.idle":"2022-04-10T18:18:33.18405Z","shell.execute_reply.started":"2022-04-10T18:18:32.399986Z","shell.execute_reply":"2022-04-10T18:18:33.183277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:33.185679Z","iopub.execute_input":"2022-04-10T18:18:33.185946Z","iopub.status.idle":"2022-04-10T18:18:33.213575Z","shell.execute_reply.started":"2022-04-10T18:18:33.185911Z","shell.execute_reply":"2022-04-10T18:18:33.21295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:33.21659Z","iopub.execute_input":"2022-04-10T18:18:33.216911Z","iopub.status.idle":"2022-04-10T18:18:33.222835Z","shell.execute_reply.started":"2022-04-10T18:18:33.216877Z","shell.execute_reply":"2022-04-10T18:18:33.221942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's a simple one input, one ouput model, with the layers in sequence, so I'll use the Sequential API","metadata":{}},{"cell_type":"code","source":"CNN_block = models.Sequential([\n    layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n    layers.BatchNormalization(),\n    layers.LeakyReLU(),\n    layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:33.224185Z","iopub.execute_input":"2022-04-10T18:18:33.224943Z","iopub.status.idle":"2022-04-10T18:18:33.241729Z","shell.execute_reply.started":"2022-04-10T18:18:33.224905Z","shell.execute_reply":"2022-04-10T18:18:33.241057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_block = models.Sequential([\n    layers.Flatten(),\n    layers.Dense(128),\n    layers.LeakyReLU(),\n])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:33.242725Z","iopub.execute_input":"2022-04-10T18:18:33.243428Z","iopub.status.idle":"2022-04-10T18:18:33.25369Z","shell.execute_reply.started":"2022-04-10T18:18:33.243392Z","shell.execute_reply":"2022-04-10T18:18:33.252781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\n\nmodel_6 = models.Sequential([\n    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirement\n    CNN_block,\n    CNN_block,\n    FC_block,\n    layers.Dense(1, activation=\"sigmoid\") # output layer\n])\nmodel_6.compile(loss='binary_crossentropy',\n               optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n               metrics=[\"accuracy\"])\nmodel_6.fit(X_train,\n           y_train,\n           epochs= 30,\n           validation_data=(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:18:33.254819Z","iopub.execute_input":"2022-04-10T18:18:33.255382Z","iopub.status.idle":"2022-04-10T18:19:40.200996Z","shell.execute_reply.started":"2022-04-10T18:18:33.255347Z","shell.execute_reply":"2022-04-10T18:19:40.200273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_6.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:19:40.20259Z","iopub.execute_input":"2022-04-10T18:19:40.202858Z","iopub.status.idle":"2022-04-10T18:19:41.130659Z","shell.execute_reply.started":"2022-04-10T18:19:40.202822Z","shell.execute_reply":"2022-04-10T18:19:41.129302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_6.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:19:41.132154Z","iopub.execute_input":"2022-04-10T18:19:41.132672Z","iopub.status.idle":"2022-04-10T18:19:41.141978Z","shell.execute_reply.started":"2022-04-10T18:19:41.132634Z","shell.execute_reply":"2022-04-10T18:19:41.141148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After a few days I give up with the the Sequential model. I figured out how to make it work with functional API, but it's be the same as re-typing the paper's code with different hyper-parameters, what is kind of pointless.","metadata":{}},{"cell_type":"markdown","source":"### Let's start again, this time I re-type the paper's github code ","metadata":{}},{"cell_type":"markdown","source":"due memory contraints I won't use the 4E-4D subsets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:25:17.37674Z","iopub.execute_input":"2022-04-10T18:25:17.377037Z","iopub.status.idle":"2022-04-10T18:25:22.880044Z","shell.execute_reply.started":"2022-04-10T18:25:17.376961Z","shell.execute_reply":"2022-04-10T18:25:22.879082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skip = 50000\n\ndata_0D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0D.csv', skiprows = skip, header=None)\ndata_1D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1D.csv', skiprows = skip, header=None)\ndata_2D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2D.csv', skiprows = skip, header=None)\ndata_3D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3D.csv', skiprows = skip, header=None)\n#data_4D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4D.csv', skiprows = skip, header=None)\n\ndata_0E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0E.csv', skiprows = skip, header=None)\ndata_1E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1E.csv', skiprows = skip, header=None)\ndata_2E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2E.csv', skiprows = skip, header=None)\ndata_3E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3E.csv', skiprows = skip, header=None)\n#data_4E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4E.csv', skiprows = skip, header=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:25:22.881987Z","iopub.execute_input":"2022-04-10T18:25:22.882309Z","iopub.status.idle":"2022-04-10T18:28:07.76655Z","shell.execute_reply.started":"2022-04-10T18:25:22.882261Z","shell.execute_reply":"2022-04-10T18:28:07.765417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data preparation","metadata":{}},{"cell_type":"code","source":"labels = {'no_unbalance':0, 'unbalance':1}\nsensor = 2 # vibration sensor 1\nwindow = 4096 \n\ndef get_windowed_features(data, label):\n    n = int(np.floor(len(data)/window))\n    data = data[:int(n)*window]\n    X = data.values.reshape((n, window))\n    y = np.ones(n)*labels[label]\n    return X,y\n\nX0,y0 = get_windowed_features(data_0D_train[sensor], \"no_unbalance\")\nX1,y1 = get_windowed_features(data_1D_train[sensor], \"unbalance\")\nX2,y2 = get_windowed_features(data_2D_train[sensor], \"unbalance\")\nX3,y3 = get_windowed_features(data_3D_train[sensor], \"unbalance\")\n#X4,y4 = get_windowed_features(data4D[sensor], \"unbalance\")\nX=np.concatenate([X0, X1, X2, X3,]) #X4])\ny=np.concatenate([y0, y1, y2, y3,]) #y4])\n\nX0_val, y0_val = get_windowed_features(data_0E_test[sensor], \"no_unbalance\")\nX1_val, y1_val = get_windowed_features(data_1E_test[sensor], \"unbalance\")\nX2_val, y2_val = get_windowed_features(data_2E_test[sensor], \"unbalance\")\nX3_val, y3_val = get_windowed_features(data_3E_test[sensor], \"unbalance\")\n#X4_val, y4_val = get_windowed_features(data4E[sensor], \"unbalance\")\nX_val=np.concatenate([X0_val, X1_val, X2_val, X3_val,]) #X4_val])\ny_val=np.concatenate([y0_val, y1_val, y2_val, y3_val,]) #y4_val])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:40:16.979253Z","iopub.execute_input":"2022-04-11T18:40:16.97983Z","iopub.status.idle":"2022-04-11T18:40:17.049619Z","shell.execute_reply.started":"2022-04-11T18:40:16.979736Z","shell.execute_reply":"2022-04-11T18:40:17.048443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape, y.shape, X_val.shape, y_val.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:28:08.93783Z","iopub.execute_input":"2022-04-10T18:28:08.938207Z","iopub.status.idle":"2022-04-10T18:28:08.945477Z","shell.execute_reply.started":"2022-04-10T18:28:08.938158Z","shell.execute_reply":"2022-04-10T18:28:08.944354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_test_ratio = 0.9\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-train_test_ratio)\nX_train = np.reshape(X_train, (X_train.shape[0], X_test.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:28:08.947294Z","iopub.execute_input":"2022-04-10T18:28:08.948021Z","iopub.status.idle":"2022-04-10T18:28:10.622082Z","shell.execute_reply.started":"2022-04-10T18:28:08.947974Z","shell.execute_reply":"2022-04-10T18:28:10.621134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:28:10.623465Z","iopub.execute_input":"2022-04-10T18:28:10.623815Z","iopub.status.idle":"2022-04-10T18:28:10.630488Z","shell.execute_reply.started":"2022-04-10T18:28:10.623755Z","shell.execute_reply":"2022-04-10T18:28:10.628933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convolutional Neural Net (CNN)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU, Dense, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, ReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.regularizers import l1_l2","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:28:10.631886Z","iopub.execute_input":"2022-04-10T18:28:10.633223Z","iopub.status.idle":"2022-04-10T18:28:11.959569Z","shell.execute_reply.started":"2022-04-10T18:28:10.633169Z","shell.execute_reply":"2022-04-10T18:28:11.958238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test set isn't shuffled, so it can be sliced into E0, E1, E2, etc. subsets \n# to check where the model performs better or worse \nX_val_1 = X_val[:len(y0_val),:,:] \nX_val_2 = X_val[len(y0_val):len(y0_val)+len(y1_val),:,:]\nX_val_3 = X_val[len(y0_val)+len(y1_val):len(y0_val)+len(y1_val)+\n                len(y2_val),:,:]\nX_val_4 = X_val[len(y0_val)+len(y1_val)+len(y2_val):len(y0_val)+\n                len(y1_val)+len(y2_val)+len(y3_val),:,:]\n#X_val_5 = X_val[len(y0_val)+len(y1_val)+len(y2_val)+len(y3_val):len(y0_val)+\n#                len(y1_val)+len(y2_val)+len(y3_val)+len(y4_val),:,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:41:37.966278Z","iopub.execute_input":"2022-04-11T18:41:37.967018Z","iopub.status.idle":"2022-04-11T18:41:37.983898Z","shell.execute_reply.started":"2022-04-11T18:41:37.966977Z","shell.execute_reply":"2022-04-11T18:41:37.983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../models'","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:28:11.990541Z","iopub.execute_input":"2022-04-10T18:28:11.991126Z","iopub.status.idle":"2022-04-10T18:28:12.01391Z","shell.execute_reply.started":"2022-04-10T18:28:11.990913Z","shell.execute_reply":"2022-04-10T18:28:12.012108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this part deals with unbalance datasets, it isn't necessary in all experiments, \n# instead of down-sampling the unbalanced subset it changes the weights of them\nweight_for_0 = len(y)/(2*len(y[y==0]))\nweight_for_1 = len(y)/(2*len(y[y==1]))\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\ndef train_models(n_conv_layers):\n    #n_conv_layers = 3 # [1,2,3,4]\n    n_dense_units = 128\n    dropout_rate = 0.0\n    use_batch_normalization = True # [True, False]\n    filter_size = 9 # [5,7,9]\n    learning_rate = 0.0001\n    n_epochs = 15 # [50,100,200]\n\n    X_in = Input(shape=(X_train.shape[1],1))\n    x = X_in\n    for j in range(n_conv_layers):\n        print(j)\n        x = Conv1D(filters=(j+1)*10,\n                   kernel_size=filter_size,\n                   strides=1,\n                   activation='linear',\n                   kernel_initializer='he_uniform')(x)\n        if use_batch_normalization:\n            x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.05)(x)\n        x = MaxPooling1D(pool_size=5, strides=2)(x)\n    x = Flatten()(x)\n    x = Dense(units = n_dense_units, activation='linear')(x)\n    x = ReLU()(x)\n    x = Dropout(rate=dropout_rate)(x)\n    X_out = Dense(units = 1, activation = 'sigmoid')(x)\n    classifier = Model(X_in, X_out)\n    classifier.summary()\n\n    best_model_filepath = f\"{model_path}/cnn_{n_conv_layers}_layers.h5\"\n    checkpoint = ModelCheckpoint(best_model_filepath, monitor='val_loss', \n                                 verbose=0, save_best_only=True, mode='min')\n    \n    classifier.compile(optimizer = Adam(learning_rate=learning_rate), loss = 'binary_crossentropy', \n                       metrics = ['accuracy'])\n\n    classifier.fit(X_train, \n                   y_train, \n                   epochs = n_epochs, \n                   batch_size = 64,\n                   validation_data=(X_test, y_test), \n                   callbacks=[checkpoint], \n                   class_weight=class_weight, verbose = 0)\n    classifier = load_model(best_model_filepath)\n    score = classifier.evaluate(X_val, y_val)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:28:12.018276Z","iopub.execute_input":"2022-04-10T18:28:12.018624Z","iopub.status.idle":"2022-04-10T18:28:12.044099Z","shell.execute_reply.started":"2022-04-10T18:28:12.018583Z","shell.execute_reply":"2022-04-10T18:28:12.04311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"There are some fine details here, like the weights for the classese of labels to deal with the unbalanced dataset later,\nthe use of for cycle and a if statement in the model,\nthe kernel initialization for the RELU layers","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nfor i in range(1,5):  #it was 1,5\n    train_models(i)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:38:06.319284Z","iopub.execute_input":"2022-04-10T18:38:06.319598Z","iopub.status.idle":"2022-04-10T18:42:30.669012Z","shell.execute_reply.started":"2022-04-10T18:38:06.319564Z","shell.execute_reply":"2022-04-10T18:42:30.667959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\naccuracies_all = []\nfor layer_n in range(1,5):\n    \n    filepath = f\"{model_path}/cnn_{layer_n}_layers.h5\"\n    model_i = load_model(filepath)\n    \n    val_acc_1 = model_i.evaluate(X_val_1, y0_val)[1]\n    val_acc_2 = model_i.evaluate(X_val_2, y1_val)[1]\n    val_acc_3 = model_i.evaluate(X_val_3, y2_val)[1]\n    val_acc_4 = model_i.evaluate(X_val_4, y3_val)[1]\n    #val_acc_5 = model_i.evaluate(X_val_5, y4_val)[1]\n    val_acc_all = model_i.evaluate(X_val, y_val)[1]\n    accuracies_layer_i = [val_acc_1, val_acc_2, val_acc_3, val_acc_4,] #val_acc_5]\n    accuracies.append(accuracies_layer_i)\n    accuracies_all.append(val_acc_all)\n\naccuracies = np.array(accuracies)\naccuracies_all = np.array(accuracies_all)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:42:30.671521Z","iopub.execute_input":"2022-04-10T18:42:30.671816Z","iopub.status.idle":"2022-04-10T18:42:43.219078Z","shell.execute_reply.started":"2022-04-10T18:42:30.671774Z","shell.execute_reply":"2022-04-10T18:42:43.217722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies_all\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:42:43.221063Z","iopub.execute_input":"2022-04-10T18:42:43.221679Z","iopub.status.idle":"2022-04-10T18:42:43.23037Z","shell.execute_reply.started":"2022-04-10T18:42:43.221615Z","shell.execute_reply":"2022-04-10T18:42:43.229225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:42:43.232785Z","iopub.execute_input":"2022-04-10T18:42:43.233368Z","iopub.status.idle":"2022-04-10T18:42:43.246118Z","shell.execute_reply.started":"2022-04-10T18:42:43.233306Z","shell.execute_reply":"2022-04-10T18:42:43.245082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's compare my simple DNN model to the paper's CNN models with the same evaluation methods","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import models, layers \n\ntf.random.set_seed(42) # for reproducibility\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel_8_DNN = models.Sequential([\n    layers.Flatten(),\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel_8_DNN.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                   metrics=\"accuracy\")\n\nhistory_8 = model_8_DNN.fit(X_train, \n                            y_train, \n                            epochs = 100, \n                            batch_size = 32,\n                            validation_data=(X_test, y_test),\n                            callbacks=[callback])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:32:51.231102Z","iopub.execute_input":"2022-04-10T18:32:51.231565Z","iopub.status.idle":"2022-04-10T18:33:13.817333Z","shell.execute_reply.started":"2022-04-10T18:32:51.231429Z","shell.execute_reply":"2022-04-10T18:33:13.816424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_i = model_8_DNN\n    \naccuracies = []\naccuracies_all = []\n\n    \nfilepath = f\"{model_path}/cnn_{layer_n}_layers.h5\"\nmodel_i = load_model(filepath)\n    \nval_acc_1 = model_i.evaluate(X_val_1, y0_val)[1]\nval_acc_2 = model_i.evaluate(X_val_2, y1_val)[1]\nval_acc_3 = model_i.evaluate(X_val_3, y2_val)[1]\nval_acc_4 = model_i.evaluate(X_val_4, y3_val)[1]\n#val_acc_5 = model_i.evaluate(X_val_5, y4_val)[1]\nval_acc_all = model_i.evaluate(X_val, y_val)[1]\naccuracies_layer_i = [val_acc_1, val_acc_2, val_acc_3, val_acc_4,] #val_acc_5]\naccuracies.append(accuracies_layer_i)\naccuracies_all.append(val_acc_all)\n\naccuracies = np.array(accuracies)\naccuracies_all = np.array(accuracies_all)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:33:13.822115Z","iopub.execute_input":"2022-04-10T18:33:13.822374Z","iopub.status.idle":"2022-04-10T18:33:17.618248Z","shell.execute_reply.started":"2022-04-10T18:33:13.82233Z","shell.execute_reply":"2022-04-10T18:33:17.616791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:33:17.621704Z","iopub.execute_input":"2022-04-10T18:33:17.623271Z","iopub.status.idle":"2022-04-10T18:33:17.632079Z","shell.execute_reply.started":"2022-04-10T18:33:17.62319Z","shell.execute_reply":"2022-04-10T18:33:17.63059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies_all","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:33:17.634811Z","iopub.execute_input":"2022-04-10T18:33:17.635185Z","iopub.status.idle":"2022-04-10T18:33:17.647594Z","shell.execute_reply.started":"2022-04-10T18:33:17.635129Z","shell.execute_reply":"2022-04-10T18:33:17.646236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"94% accuracy for the best of the paper, \n86.4% for my simple model.\n\nIt's important to note here that I ran paper's model a few times and result vary many percent points.","metadata":{}},{"cell_type":"markdown","source":"### Before we go to other models, does the window size have major effect on acc?","metadata":{}},{"cell_type":"markdown","source":"window = 2048 is the only parameter I changed","metadata":{}},{"cell_type":"markdown","source":"note: if you run the notebook from the \"Let's start again, this time reuse the paper's github code\" part it'll probably crash here without restart","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:47:19.69588Z","iopub.execute_input":"2022-04-10T18:47:19.69657Z","iopub.status.idle":"2022-04-10T18:47:19.705182Z","shell.execute_reply.started":"2022-04-10T18:47:19.6965Z","shell.execute_reply":"2022-04-10T18:47:19.703549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU, Dense, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, ReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.regularizers import l1_l2","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:47:20.142105Z","iopub.execute_input":"2022-04-10T18:47:20.142716Z","iopub.status.idle":"2022-04-10T18:47:20.153402Z","shell.execute_reply.started":"2022-04-10T18:47:20.142662Z","shell.execute_reply":"2022-04-10T18:47:20.152256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skip = 50000\n\ndata_0D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0D.csv', skiprows = skip, header=None)\ndata_1D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1D.csv', skiprows = skip, header=None)\ndata_2D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2D.csv', skiprows = skip, header=None)\ndata_3D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3D.csv', skiprows = skip, header=None)\n#data_4D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4D.csv', skiprows = skip, header=None)\n\ndata_0E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0E.csv', skiprows = skip, header=None)\ndata_1E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1E.csv', skiprows = skip, header=None)\ndata_2E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2E.csv', skiprows = skip, header=None)\ndata_3E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3E.csv', skiprows = skip, header=None)\n#data_4E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4E.csv', skiprows = skip, header=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:47:22.337361Z","iopub.execute_input":"2022-04-10T18:47:22.337681Z","iopub.status.idle":"2022-04-10T18:49:47.958135Z","shell.execute_reply.started":"2022-04-10T18:47:22.337649Z","shell.execute_reply":"2022-04-10T18:49:47.956965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = {'no_unbalance': 0, 'unbalance':1}\nsensor = 2 # first vibration sensor\nwindow = 2048\n\n\ndef get_features(data, label):\n    n = int(np.floor(len(data)/window))\n    data = data[:int(n)*window]\n    X = data.values.reshape((n, window))\n    y = np.ones(n)*labels[label]\n    return X,y\n\nX0,y0 = get_features(data_0D_train[sensor], \"no_unbalance\")\nX1,y1 = get_features(data_1D_train[sensor], \"unbalance\")\nX2,y2 = get_features(data_2D_train[sensor], \"unbalance\")\nX3,y3 = get_features(data_3D_train[sensor], \"unbalance\")\n#X4,y4 = get_features(data4D[sensor], \"unbalance\")\nX=np.concatenate([X0, X1, X2, X3,]) #X4])\ny=np.concatenate([y0, y1, y2, y3,]) #y4])\n\nX0_val, y0_val = get_features(data_0E_test[sensor], \"no_unbalance\")\nX1_val, y1_val = get_features(data_1E_test[sensor], \"unbalance\")\nX2_val, y2_val = get_features(data_2E_test[sensor], \"unbalance\")\nX3_val, y3_val = get_features(data_3E_test[sensor], \"unbalance\")\n#X4_val, y4_val = get_features(data4E[sensor], \"unbalance\")\nX_val=np.concatenate([X0_val, X1_val, X2_val, X3_val,]) #X4_val])\ny_val=np.concatenate([y0_val, y1_val, y2_val, y3_val,]) #y4_val])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:49:47.960678Z","iopub.execute_input":"2022-04-10T18:49:47.961017Z","iopub.status.idle":"2022-04-10T18:49:48.948968Z","shell.execute_reply.started":"2022-04-10T18:49:47.960968Z","shell.execute_reply":"2022-04-10T18:49:48.94787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_test_ratio = 0.9\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-train_test_ratio)\nX_train = np.reshape(X_train, (X_train.shape[0], X_test.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:49:48.950597Z","iopub.execute_input":"2022-04-10T18:49:48.950939Z","iopub.status.idle":"2022-04-10T18:49:50.677995Z","shell.execute_reply.started":"2022-04-10T18:49:48.950891Z","shell.execute_reply":"2022-04-10T18:49:50.676835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:49:50.681099Z","iopub.execute_input":"2022-04-10T18:49:50.681364Z","iopub.status.idle":"2022-04-10T18:49:50.687811Z","shell.execute_reply.started":"2022-04-10T18:49:50.681333Z","shell.execute_reply":"2022-04-10T18:49:50.686175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_for_0 = len(y)/(2*len(y[y==0]))\nweight_for_1 = len(y)/(2*len(y[y==1]))\nclass_weight = {0: weight_for_0, 1: weight_for_1}\ntf.random.set_seed(42)\n\ndef train_models(n_conv_layers):\n    #n_conv_layers = 3 # [1,2,3,4]\n    n_dense_units = 128\n    dropout_rate = 0.0\n    use_batch_normalization = True # [True, False]\n    filter_size = 9 # [5,7,9]\n    learning_rate = 0.0001\n    n_epochs = 15 # [50,100,200]\n\n    X_in = Input(shape=(X_train.shape[1],1))\n    x = X_in\n    for j in range(n_conv_layers):\n        print(j)\n        x = Conv1D(filters=(j+1)*10,\n                   kernel_size=filter_size,\n                   strides=1,\n                   activation='linear',\n                   kernel_initializer='he_uniform')(x)\n        if use_batch_normalization:\n            x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.05)(x)\n        x = MaxPooling1D(pool_size=5, strides=2)(x)\n    x = Flatten()(x)\n    x = Dense(units = n_dense_units, activation='linear')(x)\n    x = ReLU()(x)\n    x = Dropout(rate=dropout_rate)(x)\n    X_out = Dense(units = 1, activation = 'sigmoid')(x)\n    classifier = Model(X_in, X_out)\n    classifier.summary()\n\n    best_model_filepath = f\"{model_path}/cnn_{n_conv_layers}_layers.h5\"\n    checkpoint = ModelCheckpoint(best_model_filepath, monitor='val_loss', \n                                 verbose=0, save_best_only=True, mode='min')\n    \n    classifier.compile(optimizer = Adam(learning_rate=learning_rate), loss = 'binary_crossentropy', \n                       metrics = ['accuracy'])\n\n    classifier.fit(X_train, y_train, epochs = n_epochs, batch_size = 64,\n                   validation_data=(X_test, y_test), callbacks=[checkpoint], \n                   class_weight=class_weight, verbose = 0)\n    classifier = load_model(best_model_filepath)\n    score = classifier.evaluate(X_val, y_val)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:49:50.689755Z","iopub.execute_input":"2022-04-10T18:49:50.690377Z","iopub.status.idle":"2022-04-10T18:49:50.710902Z","shell.execute_reply.started":"2022-04-10T18:49:50.690326Z","shell.execute_reply":"2022-04-10T18:49:50.709765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../models'","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:49:50.712807Z","iopub.execute_input":"2022-04-10T18:49:50.713308Z","iopub.status.idle":"2022-04-10T18:49:50.725381Z","shell.execute_reply.started":"2022-04-10T18:49:50.713258Z","shell.execute_reply":"2022-04-10T18:49:50.724238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\n\nfor i in range(1,5):  #it was 1,5\n    train_models(i)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:49:50.727259Z","iopub.execute_input":"2022-04-10T18:49:50.727721Z","iopub.status.idle":"2022-04-10T18:56:42.83579Z","shell.execute_reply.started":"2022-04-10T18:49:50.727672Z","shell.execute_reply":"2022-04-10T18:56:42.834428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_1 = X_val[:len(y0_val),:]\nX_val_2 = X_val[len(y0_val):len(y0_val)+len(y1_val),:]\nX_val_3 = X_val[len(y0_val)+len(y1_val):len(y0_val)+len(y1_val)+\n                len(y2_val),:]\nX_val_4 = X_val[len(y0_val)+len(y1_val)+len(y2_val):len(y0_val)+\n                len(y1_val)+len(y2_val)+len(y3_val),:]\n#X_val_5 = X_val[len(y0_val)+len(y1_val)+len(y2_val)+len(y3_val):len(y0_val)+\n#                len(y1_val)+len(y2_val)+len(y3_val)+len(y4_val),:,:]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:56:42.837808Z","iopub.execute_input":"2022-04-10T18:56:42.838406Z","iopub.status.idle":"2022-04-10T18:56:42.850479Z","shell.execute_reply.started":"2022-04-10T18:56:42.838356Z","shell.execute_reply":"2022-04-10T18:56:42.8491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\naccuracies_all = []\nfor layer_n in range(1,5):\n    \n    filepath = f\"{model_path}/cnn_{layer_n}_layers.h5\"\n    model_i = load_model(filepath)\n    \n    val_acc_1 = model_i.evaluate(X_val_1, y0_val)[1]\n    val_acc_2 = model_i.evaluate(X_val_2, y1_val)[1]\n    val_acc_3 = model_i.evaluate(X_val_3, y2_val)[1]\n    val_acc_4 = model_i.evaluate(X_val_4, y3_val)[1]\n    #val_acc_5 = model_i.evaluate(X_val_5, y4_val)[1]\n    val_acc_all = model_i.evaluate(X_val, y_val)[1]\n    accuracies_layer_i = [val_acc_1, val_acc_2, val_acc_3, val_acc_4,] #val_acc_5]\n    accuracies.append(accuracies_layer_i)\n    accuracies_all.append(val_acc_all)\n\naccuracies = np.array(accuracies)\naccuracies_all = np.array(accuracies_all)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:56:42.852704Z","iopub.execute_input":"2022-04-10T18:56:42.853221Z","iopub.status.idle":"2022-04-10T18:57:01.778Z","shell.execute_reply.started":"2022-04-10T18:56:42.853168Z","shell.execute_reply":"2022-04-10T18:57:01.776907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:57:01.782602Z","iopub.execute_input":"2022-04-10T18:57:01.782879Z","iopub.status.idle":"2022-04-10T18:57:01.79387Z","shell.execute_reply.started":"2022-04-10T18:57:01.782828Z","shell.execute_reply":"2022-04-10T18:57:01.792328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies_all","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:57:01.795842Z","iopub.execute_input":"2022-04-10T18:57:01.796325Z","iopub.status.idle":"2022-04-10T18:57:01.808521Z","shell.execute_reply.started":"2022-04-10T18:57:01.796276Z","shell.execute_reply":"2022-04-10T18:57:01.807452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import models, layers \n\ntf.random.set_seed(42) # for reproducibility\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel_9_DNN = models.Sequential([\n    layers.Flatten(),\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(100, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel_9_DNN.compile(loss='binary_crossentropy',\n                    optimizer = tf.keras.optimizers.Adam(),\n                   metrics=\"accuracy\")\n\nhistory_9 = model_9_DNN.fit(X_train, \n                            y_train, \n                            epochs = 100, \n                            batch_size = 32,\n                            validation_data=(X_test, y_test),\n                            callbacks=[callback])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:58:36.229542Z","iopub.execute_input":"2022-04-10T18:58:36.229925Z","iopub.status.idle":"2022-04-10T18:59:19.027128Z","shell.execute_reply.started":"2022-04-10T18:58:36.229888Z","shell.execute_reply":"2022-04-10T18:59:19.026033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_i = model_9_DNN\n    \naccuracies = []\naccuracies_all = []\n\n    \nfilepath = f\"{model_path}/cnn_{layer_n}_layers.h5\"\nmodel_i = load_model(filepath)\n    \nval_acc_1 = model_i.evaluate(X_val_1, y0_val)[1]\nval_acc_2 = model_i.evaluate(X_val_2, y1_val)[1]\nval_acc_3 = model_i.evaluate(X_val_3, y2_val)[1]\nval_acc_4 = model_i.evaluate(X_val_4, y3_val)[1]\n#val_acc_5 = model_i.evaluate(X_val_5, y4_val)[1]\nval_acc_all = model_i.evaluate(X_val, y_val)[1]\naccuracies_layer_i = [val_acc_1, val_acc_2, val_acc_3, val_acc_4,] #val_acc_5]\naccuracies.append(accuracies_layer_i)\naccuracies_all.append(val_acc_all)\n\naccuracies = np.array(accuracies)\naccuracies_all = np.array(accuracies_all)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T18:59:19.029756Z","iopub.execute_input":"2022-04-10T18:59:19.030117Z","iopub.status.idle":"2022-04-10T19:00:05.915237Z","shell.execute_reply.started":"2022-04-10T18:59:19.030049Z","shell.execute_reply":"2022-04-10T19:00:05.913868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:00:05.917061Z","iopub.execute_input":"2022-04-10T19:00:05.917759Z","iopub.status.idle":"2022-04-10T19:00:05.925753Z","shell.execute_reply.started":"2022-04-10T19:00:05.917708Z","shell.execute_reply":"2022-04-10T19:00:05.924574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies_all","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:00:05.92838Z","iopub.execute_input":"2022-04-10T19:00:05.929146Z","iopub.status.idle":"2022-04-10T19:00:05.94104Z","shell.execute_reply.started":"2022-04-10T19:00:05.929092Z","shell.execute_reply":"2022-04-10T19:00:05.93964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"well, it some positive effect on the DNN network, not much on the CNN.  \n\nRunning with window size 1024 has a major negative effect on the CNN. Not suprising, because it decrease the trainable parameters too much.","metadata":{}},{"cell_type":"markdown","source":"## DNN with FFT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import RobustScaler","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:24:04.174349Z","iopub.execute_input":"2022-04-18T20:24:04.175017Z","iopub.status.idle":"2022-04-18T20:24:09.195391Z","shell.execute_reply.started":"2022-04-18T20:24:04.174889Z","shell.execute_reply":"2022-04-18T20:24:09.194603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skip = 50000\ncol_list = [2] # read only the needed columns (vibration sensor 1) \n# of the csv to save on memory\n\ndata_0D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0D.csv', \n                            skiprows = skip, header=None, usecols = col_list) \ndata_1D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1D.csv', \n                            skiprows = skip, header=None, usecols = col_list)\ndata_2D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2D.csv', \n                            skiprows = skip, header=None, usecols = col_list)\ndata_3D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3D.csv', \n                            skiprows = skip, header=None, usecols = col_list)\n#data_4D_train = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4D.csv', \n#                            skiprows = skip, header=None, usecols = col_list)\n\ndata_0E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/0E.csv', \n                           skiprows = skip, header=None, usecols = col_list)\ndata_1E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/1E.csv', \n                           skiprows = skip, header=None, usecols = col_list)\ndata_2E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/2E.csv', \n                           skiprows = skip, header=None, usecols = col_list)\ndata_3E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/3E.csv', \n                           skiprows = skip, header=None, usecols = col_list)\n#data_4E_test = pd.read_csv('/kaggle/input/vibration-analysis-on-rotating-shaft/4E.csv', \n#                           skiprows = skip, header=None, usecols = col_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:24:09.199294Z","iopub.execute_input":"2022-04-18T20:24:09.199514Z","iopub.status.idle":"2022-04-18T20:26:17.354285Z","shell.execute_reply.started":"2022-04-18T20:24:09.199482Z","shell.execute_reply":"2022-04-18T20:26:17.353525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = {'no_unbalance': 0, 'unbalance' : 1}\nwindow = 4096 \nsensor = 2 # vibration sensor 1\n\ndef get_features(data, label):\n    \"\"\"this function returns the windowed pandas series as a examples times window \n    shaped matrix\"\"\"\n    n = int(np.floor(len(data)/window))\n    data = data[:(int(n)*window)]\n    X = data.values.reshape((n, window))\n    y = np.ones(n)*labels[label]\n    return X, y\n\nX0,y0 = get_features(data_0D_train[sensor], \"no_unbalance\")\nX1,y1 = get_features(data_1D_train[sensor], \"unbalance\")\nX2,y2 = get_features(data_2D_train[sensor], \"unbalance\")\nX3,y3 = get_features(data_3D_train[sensor], \"unbalance\")\n#X4,y4 = get_features(data_4D_train[sensor], \"unbalance\")\nX=np.concatenate([X0, X1, X2, X3]) #,X4])\ny=np.concatenate([y0, y1, y2, y3]) #,y4])\n\nX0_val, y0_val = get_features(data_0E_test[sensor], \"no_unbalance\")\nX1_val, y1_val = get_features(data_1E_test[sensor], \"unbalance\")\nX2_val, y2_val = get_features(data_2E_test[sensor], \"unbalance\")\nX3_val, y3_val = get_features(data_3E_test[sensor], \"unbalance\")\n#X4_val, y4_val = get_features(data_4E_test[sensor], \"unbalance\")\nX_val=np.concatenate([X0_val, X1_val, X2_val, X3_val]) #,X4_val])\ny_val=np.concatenate([y0_val, y1_val, y2_val, y3_val]) #,y4_val])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:17.355464Z","iopub.execute_input":"2022-04-18T20:26:17.357674Z","iopub.status.idle":"2022-04-18T20:26:18.904249Z","shell.execute_reply.started":"2022-04-18T20:26:17.357635Z","shell.execute_reply":"2022-04-18T20:26:18.90345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, X_val.shape, y.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:18.90614Z","iopub.execute_input":"2022-04-18T20:26:18.907385Z","iopub.status.idle":"2022-04-18T20:26:18.916263Z","shell.execute_reply.started":"2022-04-18T20:26:18.907345Z","shell.execute_reply":"2022-04-18T20:26:18.915433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_test_split_ratio = 0.9\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 1 - train_test_split_ratio,\n                                                    random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:18.917372Z","iopub.execute_input":"2022-04-18T20:26:18.918205Z","iopub.status.idle":"2022-04-18T20:26:19.841609Z","shell.execute_reply.started":"2022-04-18T20:26:18.918167Z","shell.execute_reply":"2022-04-18T20:26:19.840738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fourier Transformation recap\nIf you have a periodic signal, like a sin() function,\nsomething the deeplearning models can't really handle\nyou can turn it into two numbers, its max size (amplitude) and that how many times\nit goes 360 degrees per second (frequency).\n\nYou can do it with any periodic signal and break it down into a combination of sin and cos functions and get amplitude and frequency for each of them.\n\nWe are dealing with a rotating weight here, and a 1D sensor, so you can be sure\nthere is at least one sin or cos in the equation.","metadata":{}},{"cell_type":"code","source":"X_fft = np.abs(np.fft.rfft(X, axis=1))[:,:int(window/2)]\nX_train_fft = np.abs(np.fft.rfft(X_train, axis=1))[:,:int(window/2)]\nX_test_fft = np.abs(np.fft.rfft(X_test, axis=1))[:,:int(window/2)]\nX_val_fft = np.abs(np.fft.rfft(X_val, axis=1))[:,:int(window/2)]\n\nX_fft[:,0]=0 # why zero out the first example?\nX_train_fft[:,0]=0\nX_test_fft[:,0]=0\nX_val_fft[:,0]=0\n                    ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:19.842823Z","iopub.execute_input":"2022-04-18T20:26:19.843262Z","iopub.status.idle":"2022-04-18T20:26:23.589993Z","shell.execute_reply.started":"2022-04-18T20:26:19.84319Z","shell.execute_reply":"2022-04-18T20:26:23.589245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input\nplt.plot(X[:,:1])\nplt.xlabel(\"Index of sample\")\nplt.ylabel(\"Signal from the vibration sensor\")\nplt.title(\"Data before FastFourierTransform\");","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:23.591137Z","iopub.execute_input":"2022-04-18T20:26:23.591387Z","iopub.status.idle":"2022-04-18T20:26:23.889345Z","shell.execute_reply.started":"2022-04-18T20:26:23.591355Z","shell.execute_reply":"2022-04-18T20:26:23.88868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the results\nplt.plot(X_fft[:,:2])\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Amplitude\")\nplt.title(\"Data after FastFourierTransform\");","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:23.890577Z","iopub.execute_input":"2022-04-18T20:26:23.89097Z","iopub.status.idle":"2022-04-18T20:26:24.11256Z","shell.execute_reply.started":"2022-04-18T20:26:23.890934Z","shell.execute_reply":"2022-04-18T20:26:24.111869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_fft.shape, X_test_fft.shape, X_val_fft.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:24.113904Z","iopub.execute_input":"2022-04-18T20:26:24.114373Z","iopub.status.idle":"2022-04-18T20:26:24.119447Z","shell.execute_reply.started":"2022-04-18T20:26:24.114335Z","shell.execute_reply":"2022-04-18T20:26:24.118744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit on training only to avoid data leakage\nscaler = RobustScaler(quantile_range=(5, 95)).fit(X_train_fft) # it excludes the outliners\n\nX_fft_sc = scaler.transform(X_fft)\nX_train_fft_sc = scaler.transform(X_train_fft)\nX_test_fft_sc = scaler.transform(X_test_fft)\nX_val_fft_sc = scaler.transform(X_val_fft)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:24.121948Z","iopub.execute_input":"2022-04-18T20:26:24.122404Z","iopub.status.idle":"2022-04-18T20:26:28.108415Z","shell.execute_reply.started":"2022-04-18T20:26:24.122369Z","shell.execute_reply":"2022-04-18T20:26:28.107481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluation","metadata":{}},{"cell_type":"code","source":"X_val_fft_1 = X_val_fft_sc[:len(y0_val),:]\ny_val_1 = y_val[:len(y0_val)]\nX_val_fft_2 = X_val_fft_sc[len(y0_val):len(y0_val)+len(y1_val),:]\ny_val_2 = y_val[len(y0_val):len(y0_val)+len(y1_val)]\nX_val_fft_3 = X_val_fft_sc[len(y0_val)+len(y1_val):len(y0_val)+\n                           len(y1_val)+len(y2_val),:]\ny_val_3 = y_val[len(y0_val)+len(y1_val):len(y0_val)+len(y1_val)+\n                len(y2_val)]\nX_val_fft_4 = X_val_fft_sc[len(y0_val)+len(y1_val)+len(y2_val):len(y0_val)+\n                           len(y1_val)+len(y2_val)+len(y3_val),:]\ny_val_4 = y_val[len(y0_val)+len(y1_val)+len(y2_val):len(y0_val)+len(y1_val)+\n                len(y2_val)+len(y3_val)]\n#X_val_fft_5 = X_val_fft_sc[len(y0_val)+len(y1_val)+len(y2_val)+len(y3_val):len(y0_val)+\n#                           len(y1_val)+len(y2_val)+len(y3_val)+len(y4_val),:]\n#y_val_5 = y_val[len(y0_val)+len(y1_val)+len(y2_val)+len(y3_val):len(y0_val)+len(y1_val)+\n#                len(y2_val)+len(y3_val)+len(y4_val)]\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:28.109593Z","iopub.execute_input":"2022-04-18T20:26:28.10983Z","iopub.status.idle":"2022-04-18T20:26:28.11818Z","shell.execute_reply.started":"2022-04-18T20:26:28.109797Z","shell.execute_reply":"2022-04-18T20:26:28.117224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Training","metadata":{}},{"cell_type":"code","source":"model_path = '../models'","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:28.119452Z","iopub.execute_input":"2022-04-18T20:26:28.119931Z","iopub.status.idle":"2022-04-18T20:26:28.127401Z","shell.execute_reply.started":"2022-04-18T20:26:28.119896Z","shell.execute_reply":"2022-04-18T20:26:28.126683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import BatchNormalization, LeakyReLU, Dense, Dropout\nfrom tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Flatten,ReLU\nfrom tensorflow.keras. optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.regularizers import l1_l2\n\n# added earlyStopCallback to the original model to save training time \n# added dropout, because at one run the model didn't generalize well, \n# I got much worse results at the evaluation, than with the test set \n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 11)\n\nweight_for_0 = len(y)/(2*len([y==0]))\nweight_for_1 = len(y)/(2*len([y==1]))\nclass_weight = {0: weight_for_0, 1 : weight_for_1}\n\nepochs = 100\nfor i in range(5):\n    X_input = Input(shape=(X_train_fft.shape[1],), name = \"input layer\")\n    x = X_input\n    for j in range(i):\n        x = Dense(units = 1024, activation=\"linear\")(x)\n        x = LeakyReLU(alpha=0.05)(x)\n        x = Dropout(0.2)(x)\n    X_output = Dense(units=1, activation=\"sigmoid\")(x)\n    model_i = Model(X_input, X_output)\n\n    best_model_file_path = f\"{model_path}/fnn_fcn_{i}_layers.h5\"\n    checkpoint = ModelCheckpoint(best_model_file_path, \n                                 monitor=\"val_loss\",\n                                 verbose=1,\n                                 save_best_only=True,\n                                 mode='min')\n    \n    model_i.compile(optimizer=Adam(learning_rate = 0.0005),\n                   loss= \"binary_crossentropy\",\n                   metrics=[\"accuracy\"]) # ,\"Recall\", \"Precision\"\n    \n    model_i.fit(X_train_fft_sc, \n                y_train,\n                epochs = epochs,\n                validation_data = (X_test_fft_sc, y_test),\n                callbacks=[checkpoint, early_stop],\n                class_weight=class_weight)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:26:28.129959Z","iopub.execute_input":"2022-04-18T20:26:28.130243Z","iopub.status.idle":"2022-04-18T20:32:44.794435Z","shell.execute_reply.started":"2022-04-18T20:26:28.130212Z","shell.execute_reply":"2022-04-18T20:32:44.793752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel_loss = []\nmodel_acc = []\nmodel_accs_per_class = []\n\nfor i in range(5):\n    best_model_filepath = f\"{model_path}/fnn_fcn_{i}_layers.h5\"\n    model_i = load_model(best_model_filepath)\n    train_acc_ges = model_i.evaluate(X_train_fft_sc, y_train)\n    val_acc_ges = model_i.evaluate(X_val_fft_sc, y_val)\n    \n    val_acc_1 = model_i.evaluate(X_val_fft_1, y_val_1)\n    val_acc_2 = model_i.evaluate(X_val_fft_2, y_val_2)\n    val_acc_3 = model_i.evaluate(X_val_fft_3, y_val_3)\n    val_acc_4 = model_i.evaluate(X_val_fft_4, y_val_4)\n#    val_acc_5 = model_i.evaluate(X_val_fft_5, y_val_5)\n    \n    model_acc.append([train_acc_ges[1], val_acc_ges[1]])\n    model_loss.append([train_acc_ges[0], val_acc_ges[0]])\n    model_accs_per_class.append([val_acc_1, val_acc_2, val_acc_3, val_acc_4])#, val_acc_5])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T20:32:44.795956Z","iopub.execute_input":"2022-04-18T20:32:44.796222Z","iopub.status.idle":"2022-04-18T20:33:23.284098Z","shell.execute_reply.started":"2022-04-18T20:32:44.796187Z","shell.execute_reply":"2022-04-18T20:33:23.283425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Binary classification trade-off\nIt's a system for prevention, but what is better, alert even when there is no unbalance (false positives can go up) or it shouldn't alert if there is unbalance (false negatives can go up).\n\nwith other word alert sometimes with nothing or not alert even if there is some change for trouble.\n\nAn interesting trade-off for any binary classification, from the problem alone\nI don't know what is better. I'd need the cost of the shutdown and the cost of the procedure to replace the part and the part itself to answer that.","metadata":{}},{"cell_type":"markdown","source":"#### Why did the FFT work so well? Domain knowledge summary\nIf you think about it...  \nThe reading of the vibration sensors is something like that:\n\nWith the rotating shaft balanced the centrifugal forces cancel out each other.  \nIf there is unbalance, thus it creates centrifugal force.  \n\nFc = m * range * angular velocity(squared) \n\nThe unbalancing weight rotates with the shaft, so in 2D,  \nthe vibration sensor pick up the 1D projection (like shadow of the clock-hand)  \nof that, so the equation changes to:  \n\nFsensor = Fc * sin(alpha) #if the sensor is in vertical position, cos(alpha) if it is horizontal  \n\nwhere alpha is the 'position' of the shaft\n\nIf you apply Fourier transformation of Fsensor, as I mentioned before  \nyou get back amplitude, what is the centrifugal force in this case. \n\nIt also implies the frequency, in this case angular speed. \n(as ang speed = 2* pi*  freq)\n\nDeeplearning models are quite bad with trigonometry, like sin, cos waves and squares.\nIn this case the four fully connected (Dense) layer manages to figure it out quite accurately.\n\nAt least this is my theory, If I'm wrong I'm happy to fix the notes here.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}